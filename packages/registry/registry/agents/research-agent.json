{
	"name": "research-agent",
	"type": "registry:agent",
	"title": "Research Agent",
	"description": "Research Agent - an AI agent",
	"dependencies": ["ai"],
	"registryDependencies": [
		"prompts:research-agent",
		"tools:glob",
		"tools:grep",
		"tools:list",
		"tools:read",
		"tools:todo"
	],
	"files": [
		{
			"path": "agents/research-agent.ts",
			"type": "registry:agent",
			"content": "import { Experimental_Agent as Agent, type LanguageModel } from \"ai\"\nimport { summarizeMessages } from \"@/agents/lib/context\"\nimport {\n\ttype EnvironmentOptions,\n\tgetEnvironmentContext,\n} from \"@/agents/lib/environment\"\nimport { getSystemPrompt } from \"@/prompts/research-agent\"\nimport { globTool } from \"@/tools/glob\"\nimport { grepTool } from \"@/tools/grep\"\nimport { listTool } from \"@/tools/list\"\nimport { readTool } from \"@/tools/read\"\nimport { createTodoTools, type TodoStorage } from \"@/tools/todo\"\n\nexport interface AgentSettings {\n\tmodel: LanguageModel\n\tcwd?: string\n\tenvironment?: EnvironmentOptions\n\ttodoStorage?: TodoStorage\n}\n\nexport async function createAgent({\n\tmodel,\n\tcwd,\n\tenvironment,\n\ttodoStorage,\n}: AgentSettings) {\n\tconst env = await getEnvironmentContext({ cwd, ...environment })\n\tconst instructions = getSystemPrompt(env)\n\tconst { todoRead, todoWrite } = createTodoTools(todoStorage)\n\tconst tools = {\n\t\tread: readTool,\n\t\tlist: listTool,\n\t\tgrep: grepTool,\n\t\tglob: globTool,\n\t\ttodoRead,\n\t\ttodoWrite,\n\t}\n\n\tconst agent = new Agent({\n\t\tmodel,\n\t\tinstructions,\n\t\ttools,\n\t\tasync prepareStep({ steps, messages }) {\n\t\t\tconst threshold = 200_000\n\t\t\tconst lastStep = steps.at(-1)\n\t\t\tconst inputTokens = lastStep?.usage?.inputTokens\n\n\t\t\tif (!inputTokens || inputTokens < threshold) {\n\t\t\t\treturn {}\n\t\t\t}\n\n\t\t\tconst summarized = await summarizeMessages(messages, model, {\n\t\t\t\tthreshold,\n\t\t\t\tkeepRecent: 8,\n\t\t\t\tprotectTokens: 40_000,\n\t\t\t})\n\t\t\treturn summarized ? { messages: summarized } : {}\n\t\t},\n\t\tproviderOptions: {\n\t\t\topenai: {\n\t\t\t\treasoningEffort: \"medium\",\n\t\t\t\treasoningSummary: \"detailed\",\n\t\t\t},\n\t\t},\n\t\tstopWhen: ({ steps }) => {\n\t\t\tif (steps.length === 0) return false\n\n\t\t\tconst lastStep = steps[steps.length - 1]\n\t\t\tif (!lastStep) return false\n\n\t\t\tif (lastStep.toolCalls && lastStep.toolCalls.length > 0) {\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\treturn true\n\t\t},\n\t})\n\n\treturn agent\n}\n"
		},
		{
			"path": "agents/lib/context.ts",
			"type": "registry:lib",
			"content": "import {\n\tgenerateText,\n\ttype LanguageModel,\n\ttype ModelMessage,\n\ttype ToolResultPart,\n} from \"ai\"\n\n/**\n * Configuration for context summarization.\n */\nexport interface SummarizeConfig {\n\t/** Token threshold that triggered summarization */\n\tthreshold: number\n\t/** Recent messages to preserve untouched */\n\tkeepRecent: number\n\t/** Tool output tokens to protect from pruning */\n\tprotectTokens: number\n\t/** Optional smaller model for summarization (cost savings) */\n\tsummaryModel?: LanguageModel\n}\n\nconst CLEARED_PLACEHOLDER = \"[Output cleared - see summary]\"\n\nfunction estimateTokens(text: string): number {\n\treturn Math.ceil(text.length / 4)\n}\n\n/**\n * Prune old tool outputs from messages while protecting recent ones.\n *\n * Walks backwards through messages, keeping the most recent tool outputs\n * (up to protectTokens worth) and clearing older ones.\n *\n * @param messages - Messages to prune (old messages only, not recent)\n * @param protectTokens - Token budget for tool outputs to keep\n */\nexport function pruneToolOutputs(\n\tmessages: ModelMessage[],\n\tprotectTokens: number,\n): { messages: ModelMessage[]; prunedCount: number } {\n\tlet protectedTokens = 0\n\tlet prunedCount = 0\n\n\t// Track which tool results to prune (by message index and part index)\n\tconst toPrune: Set<string> = new Set()\n\n\t// Walk backwards to find tool outputs\n\tfor (let i = messages.length - 1; i >= 0; i--) {\n\t\tconst msg = messages[i]!\n\t\tif (msg.role !== \"assistant\" || !Array.isArray(msg.content)) continue\n\n\t\tfor (let j = msg.content.length - 1; j >= 0; j--) {\n\t\t\tconst part = msg.content[j]\n\t\t\tif (part?.type !== \"tool-call\") continue\n\n\t\t\t// Find corresponding tool result\n\t\t\tconst resultMsg = messages.find(\n\t\t\t\t(m, idx) =>\n\t\t\t\t\tidx > i &&\n\t\t\t\t\tm.role === \"tool\" &&\n\t\t\t\t\tm.content.some(\n\t\t\t\t\t\t(p) => p.type === \"tool-result\" && p.toolCallId === part.toolCallId,\n\t\t\t\t\t),\n\t\t\t)\n\n\t\t\tif (!resultMsg || !Array.isArray(resultMsg.content)) continue\n\n\t\t\tconst resultPart = resultMsg.content.find(\n\t\t\t\t(p): p is ToolResultPart =>\n\t\t\t\t\tp.type === \"tool-result\" && p.toolCallId === part.toolCallId,\n\t\t\t)\n\n\t\t\tif (!resultPart) continue\n\n\t\t\tconst tokens = estimateTokens(JSON.stringify(resultPart.output))\n\t\t\tif (protectedTokens + tokens <= protectTokens) {\n\t\t\t\tprotectedTokens += tokens\n\t\t\t} else {\n\t\t\t\t// Mark for pruning\n\t\t\t\tconst resultMsgIdx = messages.indexOf(resultMsg)\n\t\t\t\tconst resultPartIdx = resultMsg.content.findIndex(\n\t\t\t\t\t(p) => p.type === \"tool-result\" && p.toolCallId === part.toolCallId,\n\t\t\t\t)\n\t\t\t\ttoPrune.add(`${resultMsgIdx}:${resultPartIdx}`)\n\t\t\t\tprunedCount++\n\t\t\t}\n\t\t}\n\t}\n\n\t// If nothing to prune, return original\n\tif (toPrune.size === 0) {\n\t\treturn { messages, prunedCount: 0 }\n\t}\n\n\t// Clone and prune\n\tconst pruned: ModelMessage[] = messages.map((msg, msgIdx) => {\n\t\tif (msg.role !== \"tool\" || !Array.isArray(msg.content)) {\n\t\t\treturn msg\n\t\t}\n\n\t\tconst newContent = msg.content.map((part, partIdx) => {\n\t\t\tif (toPrune.has(`${msgIdx}:${partIdx}`) && part.type === \"tool-result\") {\n\t\t\t\treturn {\n\t\t\t\t\t...part,\n\t\t\t\t\toutput: { type: \"text\" as const, value: CLEARED_PLACEHOLDER },\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn part\n\t\t})\n\n\t\treturn { ...msg, content: newContent }\n\t})\n\n\treturn { messages: pruned, prunedCount }\n}\n\nconst SUMMARIZATION_PROMPT = `You are a helpful AI assistant tasked with summarizing conversations.\n\nWhen asked to summarize, provide a detailed but concise summary of the conversation.\nFocus on information that would be helpful for continuing the conversation, including:\n- What was done\n- What is currently being worked on\n- Which files are being modified\n- What needs to be done next\n\nYour summary should be comprehensive enough to provide context but concise enough to be quickly understood.`\n\n/**\n * Generate a structured summary of old messages.\n */\nasync function generateSummary(\n\tmessages: ModelMessage[],\n\tmodel: LanguageModel,\n): Promise<ModelMessage> {\n\tconst { text } = await generateText({\n\t\tmodel,\n\t\tsystem: SUMMARIZATION_PROMPT,\n\t\tmessages: [\n\t\t\t...messages,\n\t\t\t{\n\t\t\t\trole: \"user\",\n\t\t\t\tcontent:\n\t\t\t\t\t\"Provide a detailed but concise summary of our conversation above. \" +\n\t\t\t\t\t\"Focus on information that would be helpful for continuing the conversation, \" +\n\t\t\t\t\t\"including what we did, what we're doing, which files we're working on, and what we're going to do next.\",\n\t\t\t},\n\t\t],\n\t\tmaxOutputTokens: 2000,\n\t})\n\n\treturn { role: \"assistant\", content: text }\n}\n\n/**\n * Create a fallback summary when LLM summarization fails.\n */\nfunction createFallbackSummary(messageCount: number): ModelMessage {\n\treturn {\n\t\trole: \"assistant\",\n\t\tcontent:\n\t\t\t`Previous conversation (${messageCount} messages) was summarized. ` +\n\t\t\t`Recent messages contain the current state. Continue from where we left off.`,\n\t}\n}\n\n/**\n * Summarize messages by pruning old tool outputs and generating a summary.\n * Returns null if summarization wasn't needed (not enough old messages).\n *\n * @param messages - All messages in the conversation\n * @param model - Language model to use for summarization\n * @param config - Summarization configuration\n */\nexport async function summarizeMessages(\n\tmessages: ModelMessage[],\n\tmodel: LanguageModel,\n\tconfig: SummarizeConfig,\n): Promise<ModelMessage[] | null> {\n\tconst { keepRecent, protectTokens, summaryModel } = config\n\n\t// Split messages: system + old messages vs recent messages\n\tconst systemMsg = messages[0]!\n\tconst recentCount = Math.min(keepRecent, messages.length - 1)\n\tconst oldMessages = messages.slice(1, -recentCount || undefined)\n\tconst recentMessages = recentCount > 0 ? messages.slice(-recentCount) : []\n\n\t// If not enough old messages to summarize, skip\n\tif (oldMessages.length < 3) return null\n\n\t// Prune old tool outputs\n\tconst { messages: prunedOld } = pruneToolOutputs(oldMessages, protectTokens)\n\n\t// Generate summary\n\tlet summaryMsg: ModelMessage\n\ttry {\n\t\tsummaryMsg = await generateSummary(prunedOld, summaryModel ?? model)\n\t} catch (error) {\n\t\tconsole.warn(\"Summarization failed, using fallback:\", error)\n\t\tsummaryMsg = createFallbackSummary(oldMessages.length)\n\t}\n\n\treturn [systemMsg, summaryMsg, ...recentMessages]\n}\n"
		},
		{
			"path": "agents/lib/environment.ts",
			"type": "registry:lib",
			"content": "import { execSync } from \"child_process\"\nimport { existsSync } from \"fs\"\nimport { readdir, readFile, stat } from \"fs/promises\"\nimport { homedir } from \"os\"\nimport { dirname, join } from \"path\"\n\nexport interface EnvironmentContext {\n\tworkingDirectory: string\n\tplatform: string\n\tdate: string\n\tisGitRepo: boolean\n\tfileTree?: string\n\tcustomRules?: string[]\n}\n\nexport interface EnvironmentOptions {\n\tcwd?: string\n\tincludeFileTree?: boolean\n\tfileTreeLimit?: number\n\tincludeCustomRules?: boolean\n\tcustomRuleFiles?: string[]\n}\n\nfunction detectGitRepo(cwd: string): boolean {\n\ttry {\n\t\texecSync(\"git rev-parse --is-inside-work-tree\", { cwd, stdio: \"pipe\" })\n\t\treturn true\n\t} catch {\n\t\treturn false\n\t}\n}\n\nasync function generateFileTree(cwd: string, limit = 200): Promise<string> {\n\ttry {\n\t\tconst output = execSync(\n\t\t\t\"git ls-files --cached --others --exclude-standard\",\n\t\t\t{ cwd, encoding: \"utf-8\", stdio: [\"pipe\", \"pipe\", \"pipe\"] },\n\t\t)\n\t\treturn output.trim().split(\"\\n\").filter(Boolean).slice(0, limit).join(\"\\n\")\n\t} catch {\n\t\tconst files = await walkDirectory(cwd, limit)\n\t\treturn files.join(\"\\n\")\n\t}\n}\n\nconst IGNORE_DIRS = new Set([\n\t\"node_modules\",\n\t\".git\",\n\t\"dist\",\n\t\"build\",\n\t\".next\",\n\t\"__pycache__\",\n\t\".turbo\",\n\t\".cache\",\n\t\"coverage\",\n])\n\nasync function walkDirectory(\n\tdir: string,\n\tlimit: number,\n\tprefix = \"\",\n): Promise<string[]> {\n\ttry {\n\t\tconst entries = await readdir(dir)\n\t\tconst filtered = entries.filter(\n\t\t\t(e) => !IGNORE_DIRS.has(e) && !e.startsWith(\".\"),\n\t\t)\n\n\t\tconst results = await Promise.all(\n\t\t\tfiltered.map(async (entry) => {\n\t\t\t\tconst fullPath = join(dir, entry)\n\t\t\t\tconst relativePath = prefix ? `${prefix}/${entry}` : entry\n\n\t\t\t\ttry {\n\t\t\t\t\tconst s = await stat(fullPath)\n\t\t\t\t\tif (s.isDirectory()) {\n\t\t\t\t\t\treturn walkDirectory(fullPath, limit, relativePath)\n\t\t\t\t\t}\n\t\t\t\t\treturn [relativePath]\n\t\t\t\t} catch {\n\t\t\t\t\treturn []\n\t\t\t\t}\n\t\t\t}),\n\t\t)\n\n\t\treturn results.flat().slice(0, limit)\n\t} catch {\n\t\treturn []\n\t}\n}\n\nconst DEFAULT_RULE_FILES = [\"AGENTS.md\", \"CLAUDE.md\", \"CONTEXT.md\"]\nconst GLOBAL_RULE_PATHS = [\n\tjoin(homedir(), \".config\", \"agents\", \"AGENTS.md\"),\n\tjoin(homedir(), \".claude\", \"CLAUDE.md\"),\n]\n\nasync function loadCustomRules(\n\tcwd: string,\n\tadditionalFiles: string[] = [],\n): Promise<string[]> {\n\tconst ruleFiles = [...DEFAULT_RULE_FILES, ...additionalFiles]\n\tconst found = new Set<string>()\n\tconst paths: Array<{ path: string; isGlobal: boolean }> = []\n\n\tlet dir = cwd\n\tconst root = dirname(dir)\n\twhile (dir !== root) {\n\t\tfor (const file of ruleFiles) {\n\t\t\tconst filePath = join(dir, file)\n\t\t\tif (!found.has(filePath) && existsSync(filePath)) {\n\t\t\t\tpaths.push({ path: filePath, isGlobal: false })\n\t\t\t\tfound.add(filePath)\n\t\t\t}\n\t\t}\n\t\tdir = dirname(dir)\n\t}\n\n\tfor (const filePath of GLOBAL_RULE_PATHS) {\n\t\tif (!found.has(filePath) && existsSync(filePath)) {\n\t\t\tpaths.push({ path: filePath, isGlobal: true })\n\t\t\tfound.add(filePath)\n\t\t}\n\t}\n\n\tconst results = await Promise.all(\n\t\tpaths.map(async ({ path, isGlobal }) => {\n\t\t\ttry {\n\t\t\t\tconst content = await readFile(path, \"utf-8\")\n\t\t\t\tconst prefix = isGlobal ? \"Global instructions\" : \"Instructions\"\n\t\t\t\treturn `# ${prefix} from ${path}\\n\\n${content}`\n\t\t\t} catch {\n\t\t\t\treturn null\n\t\t\t}\n\t\t}),\n\t)\n\n\treturn results.filter((r) => r !== null)\n}\n\n/**\n * Gathers rich environment context for the agent's system prompt.\n * Includes working directory, platform, git status, file tree, and custom rules (AGENTS.md, etc.).\n */\nexport async function getEnvironmentContext(\n\toptions: EnvironmentOptions = {},\n): Promise<EnvironmentContext> {\n\tconst {\n\t\tcwd = process.cwd(),\n\t\tincludeFileTree = true,\n\t\tfileTreeLimit = 200,\n\t\tincludeCustomRules = true,\n\t\tcustomRuleFiles = [],\n\t} = options\n\n\tconst isGitRepo = detectGitRepo(cwd)\n\n\tconst [fileTree, customRules] = await Promise.all([\n\t\tincludeFileTree ? generateFileTree(cwd, fileTreeLimit) : undefined,\n\t\tincludeCustomRules ? loadCustomRules(cwd, customRuleFiles) : undefined,\n\t])\n\n\treturn {\n\t\tworkingDirectory: cwd,\n\t\tplatform: process.platform,\n\t\tdate: new Date().toDateString(),\n\t\tisGitRepo,\n\t\tfileTree,\n\t\tcustomRules,\n\t}\n}\n"
		}
	]
}
